---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
# Load any R packages you need here
library(forecast)
library(ggplot2)
```

# Statement of the topic {#ch:intro}

Along with the extreme heatwave in Australia 2019-2020, one of the most devastating bushfire season in history had been witnessed. Lighting strikes and arson were been discussed among pubilc as the main cause of this disaster. This research will explore the sources of fire ignition during 2019-2020 Australia bushfires season and provide a model to predict the fire risk of neighbourhoods. Hotspots data from the JAXA’s Himawari-8 satellite and weather data from the Bureau of Meteorology of Australia will be used in ignition identification and bushfire danger estimation. In addition, an interactive web application embeds with research outcomes will be built for data visualization purpose.

## Motivation

\<left blank\>

## Background and literature review

\<left blank\>

## Research aim

The aim of this research is to use remote sensing data and apply machine learning framework to understand the sources of bushfire ignition and predict fire risk. There are 5 sub-objectives:

  1. Data integration of remote sensing data, weather data, map data and vegetation data.
  2. Develop customized clustering algorithm to classify fire clusters.
  3. Exploratory data analysis of fire clusters.
  4. Examine source of bushfire ignitions using weather records, road map and recreation site locations.
  5. Explore bushfire risk models based on weather condition, geometry information and fire history.
  6. Develop an interactive shiny app to present research outcome.

## Research plan

### Data

In order to collect necessary datasets, two methods will be performed, including retrieve files from servers and crawl data from websites. For reproducibility purpose, only public data will be used in this research. Information on the datasets can be found in Table \ref{tab:datasetinfo}. 

```{r datasets, results='asis'}
library(kableExtra)
datasets_info = data.frame(name = c("Hotspots data - JAXA’s Himawari-8 satellite",
                                    "Weather data - Bureau of Meteorology of \\\\ $\\hspace{5mm}$ Australia",
                                    "Map - OpenStreetMap",
                                    "Fuel layer - Australian Bureau of Agriculture \\\\ $\\hspace{5mm}$ and Resource Economics and Sciences"), 
                           spatialresolution = c("$0.02^\\circ \\approx 2km$",
                                                 "",
                                                 "",
                                                 ""), 
                           temporalresolution = c("Per 10 minutes",
                                                  "Daily",
                                                  "",
                                                  ""),
                           time = c("2015-2020",
                                         "2019-2020",
                                         "2020",
                                         "2018"))
knitr::kable(datasets_info, 'latex', 
             caption  = 'Data information', 
             label = "datasetinfo",
             booktabs = TRUE,
             col.names = c("Data set name", "Spatial Resolution", "Temporal resolution", "Time"),
             escape = FALSE) %>%
  kable_styling(font_size = 9)

```

### Methodology

Both supervised and unsupervised learning will be implemented to reach the research aims. 

To understand the ignition of bushfires, a customized clustering algorithm will be developed to convert hotpots data into fire history, which will contain the starting time and coordinates of each fire. This algorithm will mainly involve simulating fire growth, deciding fire boundaries controlled by tolerance and assigning hotspots data to the most probable cluster. After the clustering result being obtained, fire history will be visualized to diagnostic the performance of the algorithm. It will be done by comparing the behaviour of the same fire under different sets of hyperparameters.

Exploratory data analysis of fire history and its relative factors, like weather condition, distance to the nearest road and distance to the nearest recreation site will be performed.
Prior knowledge and featuring engineering will be needed to fully understand the relationship. We expect to discover relationships between the ignition of fire with these factors, which can help us identify the cause of bushfires later on. 

In order to examine the sources of fire ignition, different strategies will be used depending on the outcome in the previous section. If the findings from the analysis are strong and directly related to potential sources of fire ignition, hypothesis tests will be conducted to examine the pattern. If the evidence is weak, we will consider developing another clustering algorithm on fire history. This algorithm will be designed to maximize the distance between bushfire started with different causes in a high dimensional space. A probability model then can be built on top of it, which can provide a probabilistic answer for the source of bushfire ignition during 2019-2020 bushfire season.

Models for predicting fire risk of neighbourhoods will be built using raw hotspots data instead of the fire history because the hotspots data can be considered generated from a partially observable Markov decision process, and the underlying state is the development of the bushfire. From low complexity models like logistic regression to high complexity models like deep neural network will be tested.    

For sharing our research outcome, a shiny app will be built and hosted online. In addition, both static and dynamic visualization tools will be considered using. Due to the nature of Spatio-temporal data, which has at least 3-dimensional features, static map view without faceting can only provide limited information. Meanwhile, faceting map view with time will be limited by the size of caravans. Animation based map view is computationally expensive and distracting though it provides more information. Better ways for visualizing Spatio-temporal data will be explored during the development.

## Preliminary Results





\newpage

## irrelevant

## code

Included in this template is a file called `sales.csv`.  This contains quarterly data on Sales and Advertising budget for a small company over the period 1981--2005. It also contains the GDP (gross domestic product) over the same period. All series have been adjusted for inflation.  We can load in this data set using the following command:

```{r loaddata, echo=TRUE}
sales <- ts(read.csv("data/sales.csv")[,-1], start=1981, frequency=4)
```

Any data you use in your thesis can go into the data directory. The data should be in exactly the format you obtained it. Do no editing or manipulation of the data outside of R. Any data munging should be scripted in R and form part of your thesis files (possibly hidden in the output).

## Figures

Figure \ref{fig:deaths} shows time plots of the data we just loaded. Notice how figure captions and references work. Chunk names can be used as figure labels with `fig:` prefixed. Never manually type figure numbers, as they can change when you add or delete figures. This way, the figure numbering is always correct.

```{r deaths, message=FALSE, fig.cap="Quarterly sales, advertising and GDP data."}
autoplot(sales, facets=TRUE, ylab="", xlab="Year")
```

## Results from analyses

We can fit a dynamic regression model to the sales data.

```{r, echo=FALSE}
fit <- auto.arima(sales[,"Sales"], xreg=sales[,2:3], D=1)
if(!identical(fit$arma, c(1L,0L,0L,1L,4L,0L,1L)))
  stop("Model not ARIMA(1,0,0)(0,1,1)[4]")
#This is an example of how to put checks into your R code to warn you if something has gone wrong.
```

If $y_t$ denotes the sales in quarter $t$, $x_t$ denotes the corresponding advertising budget and $z_t$ denotes the GDP, then the resulting model is:
\begin{equation}
  y_t - y_{t-4} = \beta (x_t-x_{t-4}) + \gamma (z_t-z_{t-4}) + \theta_1 \varepsilon_{t-1} + \Theta_1 \varepsilon_{t-4} + \varepsilon_t
\end{equation}
where
$\beta = `r format(fit[['coef']]['AdBudget'], digits=2, nsmall=2)`$,
$\gamma = `r format(fit[['coef']]['GDP'], digits=2, nsmall=2)`$,
$\theta_1 = `r format(fit[['coef']]['ma1'], digits=2, nsmall=2)`$,
and
$\Theta_1 = `r format(fit[['coef']]['sma1'], digits=2, nsmall=2)`$.

## Tables

Let's assume future advertising spend and GDP are at the current levels. Then forecasts for the next year are given in Table \ref{tab:salesforecasts}.

```{r forecasts, results="asis"}
currentad <- rep(tail(sales[,"AdBudget"],1),4)
currentgdp <- rep(tail(sales[,"GDP"],1),4)
fc <- forecast(fit, xreg=cbind(AdBudget=currentad, GDP=currentgdp))
knitLatex::xTab(format(as.data.frame(fc), nsmall=1, digits=1),
   caption.bottom="Forecasts for the next year assuming Advertising budget and GDP are unchanged.",
   booktabs=TRUE, coldef='lrrrr',
   label='tab:salesforecasts')
```

Again, notice the use of labels and references to automatically generate table numbers. In this case, we need to generate the label ourselves.

The `knitLatex` package is useful for generating tables from R output. Other packages can do similar things including the `kable` function in `knitr` which is somewhat simpler but you have less control over the result. If you use `knitLatex` to generate tables, don't forget to include `results="asis"` in the chunk settings.

