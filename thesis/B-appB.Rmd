---
knit: "bookdown::render_book"
---

# Modified breadth-first search algorithm used in step 2 of the clustering algorithm

This modified breadth-first search algorithm includes 6 substeps:

(a) Append a randomly selected hotspot $h_i$ to a empty list $\boldsymbol{L}$, where $h_i$ was the $i$th hotspot in the interval $\boldsymbol{S}_t$.
(b) Let pointer $\boldsymbol{P}$ point to the first element of the list $\boldsymbol{L}$.
(c) Visit every $h_i \in \boldsymbol{S}_t$ where $h_i \notin \boldsymbol{L}$. Meanwhile, if $geodesic(h_i,\boldsymbol{P})\leq AdjDist$, append $h_i$ to list $\boldsymbol{L}$.
(d) Move pointer $\boldsymbol{P}$ to the next element of the list $\boldsymbol{L}$.
(e) Repeat (a) to (d) till the pointer $\boldsymbol{P}$ reach to the end of the list $\boldsymbol{L}$.
(f) Assign a new membership to all hotspots $h_i \in \boldsymbol{L}$. Repeat (a) to (f) for unassigned hotspots in interval $\boldsymbol{S}_t$.

# Effects of parameter choices in the clustering algorithm

The popularity of **DBSCAN** [@ester1996density] is partially due to the parameter tuning tools it provides. In this section, we will introduce the parameter tuning tool for our algorithm. 

There are only two parameters used in the algorithm, which are $ActiveTime$ and $AdjDist$. Increase the tolerance of undetectable time or the potential fire speed will usually reduce the total number of clusters. However, if there are large gaps between clusters spatially, increase the parameter $AdjDist$ will not significantly reduce the number of clusters. Similarly, if there are large gaps between clusters temporally, the increase of $ActiveTime$ will have limited impact on the number of clusters. In clustering algorithms, one of the metrics to measure the quality of the clustering results is the gap between clusters. Therefore, if we can find a point where the marginal effect of $ActiveTime$ and $AdjDist$ on the number of clusters is small, we may potentially obtain a reasonable choice of the parameters. Meanwhile, if the gaps are large enough, we may observe a small marginal effect when $ActiveTime$ and $AdjDist$ are over certain values. Thus, it is insufficient to pick the optimal value just by asking for a small marginal effect. Instead, we are going to check the first-order derivative of the marginal effect. The motivation is if the first-order derivative of the marginal effect is large, it means we are crossing a line where a great proportion of noisy hotspots are not seen as individual clusters anymore. 

Figure \ref{fig:ctone} and Figure \ref{fig:cttwo} show the effects of parameter choices on the number of clusters. It works like the scree plot [@cattell1966scree]. The scree plot is originally used for finding statistically significant components in the principal component analysis (**PCA**). In our application, we need to find the "elbow" of the graph to choose the value for our parameters. The "elbow" is an indication of the first-order derivative of the marginal effect is large. In Figure \ref{fig:ctone}, it's very clear we need to choose 3000km for $AdjDist$. And in Figure \ref{fig:cttwo}, we choose 24 hours for $ActiveTime$.

```{r ctone, fig.cap = "A visualization tool for parameter tuning in our algorithm. It works like a scree plot. We need to choose a point with a large second-order derivative. The reasonable choice of the parameter AdjDist is 3000km.", out.width="100%"}
knitr::include_graphics("figures/clustering_tuning_1.jpeg")
```

```{r cttwo, fig.cap = "A visualization tool for parameter tuning in our algorithm. It works like a scree plot. We need to choose a point with a large second-order derivative. The reasonable choice of the parameter ActiveTime is 24 hours.",out.width="100%"}
knitr::include_graphics("figures/clustering_tuning_2.jpeg")
```

# Hyperparameter tuning for ignition classifiers

```{r}
tibble(Hyperparameter = c("Multinomial logistic regression", "decay"), 
       Range = c("", "{0.2h|h = 1, 2, ..., 10}")) %>%
  bind_rows(tibble(Hyperparameter = c("Random forest", "mtry"), 
       Range = c("", "{h|h=1, 2, ..., 10}"))) %>%
  bind_rows(tibble(Hyperparameter = c("XGBoost", "max_depth", "nrounds", "eta", "subsample", "gamma", "colsample_bytree", "min_child_weight"), 
       Range = c("", "{3, 5, 7, 9}", "{50h|h = 1, 2, ..., 200}", "{0.3, 0.2, 0.1, 0.05, 0.025, 0.0125, 0.00625}", "{0.05h|h = 10, 11, ..., 18}", "{0.2h|h = 0, 1, ..., 5}", "{0.05h|h = 10, 11, ..., 18}", "{1, 3, 5, 7}"))) %>%
  knitr::kable('latex',
               booktabs = T,
               label = 'hpgrid',
               caption = "A grid of values tested in hyperparameter tuning for each candidate model.")  %>%
  row_spec(c(2, 4), hline_after = T) %>%
  kableExtra::add_indent(c(2, 4, 6:12))
```


(ref:hpgridtwodetail) A description of each hyperparameter. The definition of the hyperparameters is referenced from the documentations of the package `nnet` [@R-nnet], package `randomForest`[@R-rf] and package `xgboost` [@R-xgboost].

```{r}
tibble(Hyperparameter = c("Multinomial logistic regression", "decay"), 
       Description = c("", "Parameter for weight decay")) %>%
  bind_rows(tibble(Hyperparameter = c("Random forest", "mtry"), 
       Description = c("", "Number of variables randomly sampled as candidates at each split"))) %>%
  bind_rows(tibble(Hyperparameter = c("XGBoost", "max_depth", "nrounds", "eta", "subsample", "gamma", "colsample_bytree", "min_child_weight"), 
       Description = c("", "Maximum depth of a tree", "The number of rounds for boosting", "Step size shrinkage used in update to prevents overfitting", "Subsample ratio of the training instances", "Minimum loss reduction required to make a further partition on a leaf node of the tree", "subsample ratio of columns when constructing each tree", "Minimum sum of instance weight (hessian) needed in a child"))) %>%
  knitr::kable('latex',
               booktabs = T,
               label = 'hpgridtwo',
               caption = "(ref:hpgridtwodetail)")  %>%
  row_spec(c(2, 4), hline_after = T) %>%
  kableExtra::add_indent(c(2, 4, 6:12)) %>%
  kableExtra::kable_styling(latex_options = "scale_down")
```

```{r}
tibble(Hyperparameter = c("Multinomial logistic regression", "decay"), 
       Value = c("", "0.2")) %>%
  bind_rows(tibble(Hyperparameter = c("Random forest", "mtry"), 
       Value = c("", "1"))) %>%
  bind_rows(tibble(Hyperparameter = c("XGBoost", "max_depth", "nrounds", "eta", "subsample", "gamma", "colsample_bytree", "min_child_weight"), 
       Value = c("", "5", "4800", "0.025", "0.85", "0.8", "0.55", "1"))) %>%
  knitr::kable('latex',
               booktabs = T,
               label = 'hpgridthree',
               caption = "The final result of the hyperparameter tuning.")  %>%
  row_spec(c(2, 4), hline_after = T) %>%
  kableExtra::add_indent(c(2, 4, 6:12))
```

# Model performance


```{r}
data.frame(Lightning = c(568,182,22,5,777), 
           Accident = c(259,277,77,21,634),
           Arson = c(74,120,120 ,11, 325),
           Burning_off = c(33,51,30,22, 136),
           Total = c( 934, 630, 249, 59, 1872)) %>%
   mutate(Lightning = c(paste0(Lightning[1:4], " (", round(Lightning[1:4]/Lightning[5]*100, 1), "%)" ), paste0(Lightning[5]))) %>%
  mutate(Accident = c(paste0(Accident[1:4], " (", round(Accident[1:4]/Accident[5]*100, 1), "%)" ), paste0(Accident[5]))) %>%
  mutate(Arson = c(paste0(Arson[1:4], " (", round(Arson[1:4]/Arson[5]*100, 1), "%)" ), paste0(Arson[5]))) %>%
  mutate(Burning_off = c(paste0(Burning_off[1:4], " (", round(Burning_off[1:4]/Burning_off[5]*100, 1), "%)" ), paste0(Burning_off[5]))) %>%
  `row.names<-`(c("Prediction:Lightning", "Prediction:Accident", "Prediction:Arson", "Prediction:Burning_off", "Total")) %>%
  knitr::kable('latex',
               booktabs = T,
               caption = "Confusion matrix of multinomial logit model. The overall accuracy was 0.5272.")
```

```{r}
data.frame(Lightning = c(663,74,31,9,777), 
           Accident = c(114,434,72,14,634),
           Arson = c(64,106,144 ,11, 325),
           Burning_off = c(45,33,30,28, 136),
           Total = c( 934, 630, 249, 59, 1872)) %>%
  
   mutate(Lightning = c(paste0(Lightning[1:4], " (", round(Lightning[1:4]/Lightning[5]*100, 1), "%)" ), paste0(Lightning[5]))) %>%
  mutate(Accident = c(paste0(Accident[1:4], " (", round(Accident[1:4]/Accident[5]*100, 1), "%)" ), paste0(Accident[5]))) %>%
  mutate(Arson = c(paste0(Arson[1:4], " (", round(Arson[1:4]/Arson[5]*100, 1), "%)" ), paste0(Arson[5]))) %>%
  mutate(Burning_off = c(paste0(Burning_off[1:4], " (", round(Burning_off[1:4]/Burning_off[5]*100, 1), "%)" ), paste0(Burning_off[5]))) %>%
  `row.names<-`(c("Prediction:Lightning", "Prediction:Accident", "Prediction:Arson", "Prediction:Burning_off", "Total")) %>%
  knitr::kable('latex',
               booktabs = T,
               caption = "Confusion matrix of GAM model. The overall accuracy was 0.6779.")
```

```{r}
data.frame(Lightning = c(695,53,22,7,777), 
           Accident = c(87,465,72,10,634),
           Arson = c(42,85,183 ,15, 325),
           Burning_off = c(36,38,22,40, 136),
           Total = c( 934, 630, 249, 59, 1872)) %>%
  
   mutate(Lightning = c(paste0(Lightning[1:4], " (", round(Lightning[1:4]/Lightning[5]*100, 1), "%)" ), paste0(Lightning[5]))) %>%
  mutate(Accident = c(paste0(Accident[1:4], " (", round(Accident[1:4]/Accident[5]*100, 1), "%)" ), paste0(Accident[5]))) %>%
  mutate(Arson = c(paste0(Arson[1:4], " (", round(Arson[1:4]/Arson[5]*100, 1), "%)" ), paste0(Arson[5]))) %>%
  mutate(Burning_off = c(paste0(Burning_off[1:4], " (", round(Burning_off[1:4]/Burning_off[5]*100, 1), "%)" ), paste0(Burning_off[5]))) %>%
  `row.names<-`(c("Prediction:Lightning", "Prediction:Accident", "Prediction:Arson", "Prediction:Burning_off", "Total")) %>%
  knitr::kable('latex',
               booktabs = T,
               caption = "Confusion matrix of XGBoost model. The overall accuracy was 0.7388.")
```

# Error rate map

In order to explore the prediction performance of the final model on the test set, we plot an error rate map to reveal the spatial patterns. The plot is given in Figure \ref{fig:errormap}. From the error rate map, we can observe that our model correctly predicts most of the cases in the east of Victoria, which is the mountain area. However, it performs worse near the Melbourne region. Besides, our model does not fit well on the boundary of the north-west of Victoria.

```{r errormap, fig.cap="The spatial patterns of the error rate of the final model. We omit regions with less than 5 bushfires occurred. Our model makes very few mistakes in the east of Victoria but has a higher error rate near Melbourne."}
  set.seed(10086)
  
  # Read in training data
  training <- read_csv("data/training.csv")
  
  training <- training %>%
    filter(!CAUSE %in% c("BURNING BUILDING",
                         "WASTE DISPOSAL, INDUSTRIAL, SAWMILL, TIP",
                         "WASTE DISPOSAL, DOMESTIC",
                         "BURNING VEHICLE, MACHINE",
                         "BURNING BUILDING")) %>%
    filter(new_cause != "other") %>%
    filter(new_cause != "relight")
  
  
  training <- select(training, -c(EVENTID:FIRE_NUM), -id, -CAUSE, -FOREST, -FOR_CODE, -FOR_CAT)
  
  training <- mutate(training,
                     year = factor(year(FIRE_START)),
                     month = factor(month(FIRE_START), levels = c(10,11,12,1,2,3)),
                     day = factor(day(FIRE_START), levels = c(1:31)),
                     wod = factor(wday(FIRE_START), levels = c(1:7)))
  
  training <- filter(training, month %in% c(10,11,12,1,2,3))
  
  training <- na.omit(training)
  
  training <- mutate(training, new_cause = ifelse(new_cause == "accidental_human", "accident", new_cause)) %>%
    mutate(new_cause = ifelse(new_cause == "burning_off_human", "burning_off", new_cause)) %>%
    mutate(new_cause = factor(new_cause)) %>%
    mutate(FOR_TYPE = factor(FOR_TYPE))
  
  training <- na.omit(training)
  
  training <- mutate(training,
                     log_dist_cfa = log(dist_cfa),
                     log_dist_camp = log(dist_camp),
                     log_dist_road = log(dist_road),
                     COVER = factor(COVER),
                     HEIGHT = factor(HEIGHT))
  
  training <- rename(training, cause = new_cause)
  training <- mutate(training,
                     cause = fct_relevel(cause,
                                         "lightning",
                                         "accident",
                                         "arson",
                                         "burning_off"))
  
  training <- na.omit(training)
  
  training <- select(training, -year, -dist_road, -dist_cfa, -dist_camp, -FIRE_START)
  
  inTraining <- createDataPartition(training$cause, p = .8, list = FALSE)[,1]
  train_set <- training[inTraining,]
  test_set  <- training[-inTraining,]
  
  rf_best_features <- readRDS("data/RF_best_features.rds")
  
  rf_model <- readRDS("data/RF_model.rds")

  
  test_set$error <- predict(rf_model, newdata = test_set) != test_set$cause
  
library(sf)
library(rnaturalearth)
au_map <- ne_states(country = 'Australia', returnclass = 'sf')
vic_map <- au_map[7,]

lat <- seq(34, 39, 0.5)
lat <- -lat
lon <- seq(141, 150, 0.5)

grids <- expand.grid(lat, lon)


rect <- function(x){

  # left top
  lat1 <- x[1]
  lon1 <- x[2]

  # right top
  lat2 <- x[1]
  lon2 <- x[2]+0.5

  # right bottom
  lat3 <- x[1]-0.5
  lon3 <- x[2]+0.5

  # left bottom
  lat4 <- x[1]-0.5
  lon4 <- x[2]

  st_sfc(st_polygon(list(matrix(c(lon1,lat1,lon2,lat2,lon3,lat3,lon4,lat4,lon1,lat1), ncol =2, byrow = TRUE))))

}

rect_list <- apply(grids[1:nrow(grids),],1,rect)

rect_list <- do.call(c, rect_list)

st_crs(rect_list) <- 4326

indexes <- st_intersects(vic_map$geometry, rect_list)[[1]]
rect_list <- rect_list[indexes]

test_set2 <- st_as_sf(filter(test_set, error), coords = c("lon", "lat"), crs = 4326)
test_set3 <- st_as_sf(test_set, coords = c("lon", "lat"), crs = 4326)

temp <- unlist(lapply(st_intersects(rect_list, test_set3), length))
temp <- ifelse(temp <= 4, NA, temp) 


as.data.frame(rect_list) %>%
  mutate(error = unlist(lapply(st_intersects(rect_list, test_set2), length))/temp) %>%
  mutate(error = ifelse(is.nan(error), NA, error)) %>%
  na.omit() %>%
  ggplot() +
  geom_sf(aes(geometry = geometry, fill = error)) +
  geom_sf(data = vic_map, fill = NA, col = "black") +
  scale_fill_distiller(palette = "Reds", direction = 1, limits = c(0,1)) +
  theme_map() +
  theme(legend.position = "right") +
  labs(fill = "Error rate")

```

# Supplementary material

Supplementary materials include figures, codes and documentations can be found in the Github repository of this project https://github.com/TengMCing/bushfire-paper
